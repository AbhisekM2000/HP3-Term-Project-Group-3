# Time Profiling - Introduction
The time profiling setup, as mentioned in the main Readme, runs a forward pass of the VGG and AlexNet architecture over multiple batch-sizes for all the convolution algorithms multiple times each (= 3 as default), measures the different times as convolution time, algorithmic overhead time and total time and compares them.

# Time Calculation
In order to capture run time of the different convolution methods, we use CUDA Events API and add timing logic within each kernel. At each convolution layer, we break the time taken by the API function into 3 time metrics:
* ***Convolution Time*** - This includes the time taken for all the crucial operations/kerels that are involved in the main convolution logic [The part that multiplies the matrices and computes the answert]
* ***Pre/post process Overhead Time*** - This includes the algorithmic overheads incurred due to functions/kernels for preprocessing like padding, filter size manipulation, etc. or for postprocessing like cropping, striding the output out of a bigger array or longer C++ copy operations. This **DOES NOT** include times for CUDA memory operations like cudaMemcpy, cudaMalloc, etc. 
	

	> **Note** - Overhead Time for Im2Col Kernel was assumed to be 0.01 for the sake of graph plotting. In real, since there are no explicit pre/post-processing steps, the overhead by our definition is 0 


* ***Total Time*** - This is the total time taken for the function call which includes Convolution Time, Pre/post process Overhead Time **AS WELL AS** the time taken for memory operations like CudaMalloc, etc.
Total Time = Convolution Time + Pre/postprocess Overhead Time +  Any other overhead due to memory operations

> **Note** - Convolution and Overhead Time for CUDNN API Call were assumed to be 0 since we don't know how the split is internally. Only Total Time is considered

# Experimental Setup
* **Logger** [`timeProfiler.cc`] 
	This C++ script takes the following inputs:
	* Mode - "VGG"/"ALEX"
	* Path to Pretrained Model
	* Number of  runs for each kernel and batchsize setting
	* Algorithm - "DIRECT"/"FFT"/"IM2COL"/"WINOGRAD"/"CUDNN[default]"

	With this input, it runs a forward pass of the inferencing engine for the given architecture for batchsizes = 1, 2, 4 and 8. For each setting, the forward pass is run < Number of Runs > times.
	 
* **Analyzer** [`analyzer.py`]
	Takes the logfiles generated by the logger and generates plots
