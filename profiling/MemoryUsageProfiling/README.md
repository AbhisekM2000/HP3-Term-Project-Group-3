
# Memory Usage Profiling - Introduction

This profiling setup runs a forward pass of the VGG architecture for a single image and profiles the memory being used through ``nvidia-smi`` command. It then plots a comparison of the GPU usage pattern for the different kernels. 
  

# Measuring GPU Usage
In order to measure the GPU usage, we take a naive, yet effective, approach (and that is why this is an experimental profiler) and use the `nvidia-smi` command to log the GPU memory being used at regular intervals

**Command Used:**
```
$ nvidia-smi --query-gpu=memory.used --format=csv -lms 10 > memory_<KERNEL NAME>.txt &
```

We run this command in the background before running the forward pass inferencing and the memory usage is logged in the memory_< Kernel Name>.txt file. 

>  **Note** - Due to the requirement of running background processes, this will not work on Colab 
  

# Experimental Setup
*  **Logger**  [`gpuMemProfiler.cc` and `memProfiler.sh` ]
	The C++ scripts just runs a forward pass of the VGG-19 architecture for 1 image with the given algorithm
The bash script is the main logger which before running the forward pass binary generated by the above script, issues the nvidia-smi command for profiling and kills it after the run for each algorithm

*  **Analyzer**  [`memoryAnalyzer.py`]
	It takes the logfiles generated by the logger and generate the memory usage plot
